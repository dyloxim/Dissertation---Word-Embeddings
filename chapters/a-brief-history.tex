\chapter{A Brief History of Word Embeddings}\label{chapter:a-brief-history}

\section{Counting Models}


\subsection{early theory}
meaning in pre-19th century thought~\cite{meier_oeser_2019_meaning_in}

harris -distributional structure:~\cite{harris_1954_distrib_struct}

``you will know a word by the company it keeps''~\cite{firth_1957_a_syn_of_lin}

hinton -distributed:~\cite{hinton1984distributed}

Quine -two dogmas of empiricism:~\cite{quine_1951_main_trends}

wittgenstein -meaning is use:~\cite{wittgenstein53_philos}

VSM overview:~\cite{turney10-from-frequen-to-meanin}

another VSM overview:~\cite{baroni-2010-distributional-memory}


\subsection{context: Early NLP .. document indexing}
bates: models of natural language understanding~\cite{bates-1995-models}

luhn: a statistical approach~\cite{luhn_1957_a_statistical_approach}

maron: on relevance~\cite{maron-1960-on-relevance}

looking back at ``on relevance''~\cite{thompson-2008-looking-back}

relevance weighting search terms:~\cite{robertson_1976_relevance_weighting}

contextual correlates of semantic similarity:~\cite{miller-1991-contextual-correlates}

brown clustering n grams:~\cite{brown-1990-class-based-ngram}

\subsection{latent semantic analysis}
using lsa to improve document indexing->~\cite{dumais-1988-using-lsa-to-improve}

indexing by lsa->~\cite{deerwester-1990-indexing-by-lsa}

a solution to plato's problem->~\cite{landauer_1997_a_solution}

introduction to lsa->~\cite{landauer_1998_an_introduction_lsa}

r package -lsa~\cite{wild-2007-investigating-unstructured-texts-with-lsa}

2007 overview:~\cite{wild-2007-investigating-unstructured-texts-with-lsa}


\subsection{weighting methods}
mutual information:~\cite{church-hanks-1990-word}

term weighting approaches (1988)~\cite{salton_1988_term_weighting_approaches}

measuring semantic entropy~\cite{melamed_2002_measuringsemantic}

\section{Predicting Models}
context: words as input features..one hot encodings

\subsection{overview}
primer on neural network models in natural language processing:~\cite{goldberg15_primer_neural_networ_model_natur_languag_proces}

deep learning book:~\cite{goodfellow-et-al-2016-deep-learning}


\subsection{early work}
learning representations by back propogating errors (1986)~\cite{rumelhart-1986-learning-representations}
collobert 2011 NLP (almost) from scratch:~\cite{collobert-2011-natural}

bengio 2003:~\cite{bengio-2003-a-neural-prob-lang-model}

simple and general method for semi-supervised learning (2010):~\cite{turian-etal-2010-word}

collobert-western:~\cite{collobert-2008-a-unified-architecture}

hierarchical model:~\cite{morin-2005-hierarchical-probabilistic}

\subsection{word2vec models}
efficient estimations:~\cite{mikolov13_effic_estim_word_repres_vector_space}

phrases and their composability:~\cite{mikolov13_distr_repres_words_phras_their_compos}

word2vec explained:~\cite{goldberg14_word2_explain}


\section{Relation Between Model Types \& Further Developments}
dont count predict!~\cite{baroni-etal-2014-dont}

neural word embeddings as implicit matrix factorisation:~\cite{levy-2014-neural-WE-as}

hyperparameters in LSA:~\cite{levy-2015-improving-distributional-similarity}

NCE:~\cite{mnih-2013-NCE}


salle papers about improving count models with ideas from predict models:

~\cite{salle16_enhan_lexvec_distr_word_repres}

~\cite{salle-etal-2016-matrix}

~\cite{salle-2018-incorporating}

other approach to improving count methods:~\cite{stratos-2015-model-based-word}

