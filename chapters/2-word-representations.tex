\chapter{Word Representations}
The embedding algorithms discussed in this report represent the confluence of two key ideas: first that the meaning of a word corresponds with the contexts in which it occurs, and second that the meaning of a word can be represented by a vector of numerical values.

This first idea is called the \textbf{distributional hypothesis}, was ``widespread in linguistic theory in the 1950s'' Jurafsky (2021), with key contributions to its conception contributed by Joos (1950) \emph{Description of Language Design}, Harris (1954) \emph{Distributional Structure}, and Firth (1957) \emph{A synopsis of linguistic theory 1930-1955}.

\begin{quote}
To illustrate the distributional hypothesis, consider for example the possible meanings of ``bank'' discussed above in relation to polysemy: \emph{bank} as in the organisation, and \emph{bank} as in riverside. The first sense of the word is identified with neighboring word contexts like: account, manager, desk, etc. whereas the second sense is distinguished by context words like: river, grassy, marsh.
\end{quote}

The way these distributional relationships are inferred by analysis of a \textbf{corpus} (a collection of reference texts) and synthesised in vector form differ according to the embedding algorithm used. The `count models' developed in the 1960s and refined thereafter were the first to apply the distributional hypothesis to create word vectors, and remained the state of the art until a series of papers published by Milokov et al (2013) presented efficient methods for generating word vectors using neural networks, along with a standard software implementation $\texttt{word2vec}$ which then became ubiquitous.

We will discuss count models first as they lay the contextual foundation for the innovations of the later predict models.

\section{Count Models}
\subsection{History}
The adoption of distributional perspectives within linguistic spheres in the 1950s coincided with an emerging interest in mechanical and computational methods for document indexing and information retrieval (IR); this purpose was one of the first applications considered for modern computers. The question of quantifying document `aboutness' and relatedness suggested, as viewed through the right mathematical lens, a notion of a `document space' populated by `document vectors' where relatedness could be quantified by a distance function on the space. From this the notion word vectors naturally followed. We summarise some key events in this lineage below.

In 1948 two chemists and leading advocates of punched cards, James W. Perry and G. Malcolm Dyson, petitioned IBM head, Thomas Watson, Sr to invest in research \& development on ``mechanical solutions for scientistsâ€™ specific information problems'', who selected Hans Peter Luhn for the task [cite: Rieder, B 2020].

This appointment resulted in ``a remarkable series of papers'' [cite: salton 1987] published by Luhn between 1957 \& 1959, in particular: \emph{A Statistical Approach to Mechanized Encoding and Searching of Literary Information}, Luhn (1957). The concepts discussed in this paper were prescient of future developments in the mechanical processing of documents but details of computation are omitted.

Maron \& Kuhns (1960) provided details of a potential approach to the computation of such a system in the influential \emph{On relevance, Probabilistic Indexing and Information Retrieval.} published around the same time. Crucially, the paper suggests the use of conditional probabilities to compute `closeness of index terms'. This was an application of distributional thinking to quantify word similarity, but it did not specify a vector-space conception of the problem.

This final piece was supplied by Paul Switzer in \emph{Vector Images in Document Retrieval} (1964), which references Maron \& Kuhn's paper and elaborates by suggesting the construction of a `term-term association matrix' based on co-occurence frequencies producing an `index term vector space' in which documents are then located. It is the first paper to suggest that document similarity could be determined by projecting index terms and documents into a vector space.


Details of the resulting model and the most widely adopted refinements are the focus of this section.

\subsection{Preliminaries}
We begin by defining terms already encountered in mathematical terms.

\begin{definition}[Orthography]\label{def:orthography}
  An orthography is a finite set of symbols.
\end{definition}

\begin{example}\label{ex:orthographies}
  The sets:
  \begin{align}
    &\mathscr{O}_L=\{c\mid\text{$c$ is a symbol that can be copied and pasted the pdf of this report}\}\label{eq:orth-doc}\\
    &\mathscr{O}_P=\{c\mid\text{$c$ is the inscription of a regular polygon with eight sides or less}\}\label{eq:orth-shape}\\
    &\mathscr{O}_L=\{c\mid\text{$c$ is a typable character marked on \autoref{fig:uk-keyboard}}\}\label{eq:orth-keyboard}
  \end{align}
  are all examples of orthographies.
\end{example}
\vspace{6pt}

\begin{figure}[!ht]
 \centering
 \includesvg[width=0.9 \columnwidth]{svg-inkscape/uk-keyboard.svg}
 \caption{A standard uk keyboard layout}
 \label{fig:uk-keyboard}
\end{figure}

Note, the term `orthography' in linguistics refers to the conventions of written language in general and is distinct from the use of `an orthography' we use here.

\begin{definition}[document]
  A document $d$ is a finite sequence $c_0,c_1,\dots,c_n$ ($c\in\mathscr{O}$), where $\mathscr{O}$ is an orthography.
\end{definition}

\begin{example}[This Report]\label{ex:doc-report}
    The sequence of symbols obtained by copy pasting this entire report into a text editor (ordered by starting with the cursor at the top of the file and moving the cursor with the arrow keys one position to the right at a time -including newlines) is a document using the orthography \ref{eq:orth-doc}.
\end{example}
\vspace{6pt}

\begin{example}[Shapes Document]\label{ex:doc-shapes}
    The sequence of shapes: \emph{triangle, triangle, square, triangle, triangle, octagon} is a document using the orthograpy \ref{eq:orth-shape}.
\end{example}
\vspace{6pt}

\begin{example}[Plato Documents]\label{ex:doc-plato}
    Any of the works of Plato available at the \href{https://www.gutenberg.org/ebooks/author/93}{Project Gutenberg} website, copied and pasted from their plain text formats and using the same procedure described above, are documents using the orthography \ref{eq:orth-keyboard}.
\end{example}

\begin{definition}[Word-Token]
  A word token $w$ has the same attributes as a document; it is a finite sequence $c_0,c_1,\dots,c_n$ ($c\in\mathscr{O}$), where $\mathscr{O}$ is an orthography. We only refer to sequences of symbols as word tokens when they are the output of a tokenizer, however.
\end{definition}

\begin{definition}[Tokenizer]
   A tokenizer $T$ is a function that maps a document to a sequence of word tokens; $T(d)=w_1,w_2,\dots,w_n$. The output is refered to as the 'tokenized' form of the document. We use the shorthand $W_d$ to refer to the tokenized document $T(d)$ when it is clear which tokenization has been used.\footnote{Here we consider tokenization in an abstract sense essentially saying ``A tokenizer is a function that extracts tokens''. A practical discussion of tokenization and text-preprocessing is found in \autoref{sec:preprocessing}.
\end{definition}

\begin{example}[This Report Tokenized]\label{ex:doc-report-t}
  The document from \autoref{ex:doc-report} if tokenized by treating everything occuring between two spaces (or between a space and the start or end of a line) yields a sequence of word tokens beginning \emph{``THE'' ``UNIVERSITY'' ``OF''}.
\end{example}
\vspace{6pt}

\begin{example}[Shapes Document Tokenized]\label{ex:doc-shapes-t}
  The document from \autoref{ex:doc-shapes} if tokenized using the function:
  \begin{align*}
    &T(d):\quad \texttt{Group repeated symbols in the sequence $d$ into word-tokens}\\
    &\hphantom{T(d):\ }\texttt{(in the order of occurrence).}
  \end{align*}
  produces the output: \emph{``triangle triangle'', ``square'', ``triangle triangle'', ``octagon''}
\end{example}
\vspace{6pt}

\begin{definition}[Corpus]
  A corpus $C$ is a finite set of documents $\{d_1, d_2,\dots,d_n\}$.
\end{definition}

\begin{example}[Plato Corpus]\label{ex:corp-plato}
  All the of Plato works referred to in \autoref{ex:doc-plato}, if converted to documents using the same procedure described there, considered together form a corpus of documents. We will refer to this corpus as $C_p$.
\end{example}

\begin{definition}[Vocabulary]
  The vocabulary of a tokenized document $W_d$ is the set of unique word tokens in the tokenization of that document; $V(W_d)=\{w\mid w\in\, \text{the sequence $W_d$}\}$. The vocabulary of a corpus is the union of the vocabularies of each document in that corpus; $V(C)=V(d_1)\cup V(d_2)\cup \dots\cup V(d_n)$.
\end{definition}

\begin{example}[Shapes Document Vocabulary]
  Using the same tokenization as before, the document from \autoref{ex:doc-shapes} has the vocabulary:
  \begin{align*}
    &V(\text{\emph{``triangle triangle'', ``square'', ``triangle triangle'', ``octagon''}})\\
    &\quad=\{\text{\emph{``triangle triangle''}},\, \text{\emph{``square''}},\, \text{\emph{``octagon''}}\}$$
    \end{align*}
\end{example}
\vspace{6pt}

\begin{definition}[$\#$ Operator]
  The $\#$ symbol is used to denote the number of elements of the object it precedes. Before sequences it counts the number of elements in the sequence: $\#W_d=n$ (for $W_d=w_1,\dots,w_n$) \footnote{This is like the 'word count' feature many text editors provide}. Before sets it is a shorthand for the size of that set: $\#V(d)=|V(d)|=\text{the number of words in the vocabulary $V(d)$}$. Before a corpus it counts the number of documents in that corpus.
\end{definition}

\begin{example}[Shapes Document Sizes]
  The word count of the tokenized shapes document $$W_d=\text{\emph{``triangle triangle'', ``square'', ``triangle triangle'', ``octagon''}}$$ is $\#W_d=4$ and the vocabulary size is $\#V(W_d)=3$.
\end{example}
\vspace{6pt}

\begin{example}[Plato Document Sizes]
  A selection of the Plato documents from \ref{ex:doc-plato}, tokenized using the simple method: remove the characters $\texttt{.,?!:;()[]*}$, and treat spaces and newlines as word boundaries\footnote{the code used to create these examples and others examples on the same corpus is available in the appendix}, have the following word counts and vocabulary sizes:
  \vspace{6pt}
  \begin{center}
    \begin{tabular}{|| c | c c c c c c ||}
      \hline
      Document ($d$) & $\texttt{laws}$ & $\texttt{republic}$ & $\texttt{cratylus}$ & $\texttt{meno}$ & $\texttt{apology}$ & $\texttt{crito}$\\
      \hline\hline
      $\#W_d$ & 140,406 & 118,283 & 23,883 & 12,716 & 11,392 & 5,332\\
      \hline
      $\#V(W_d)$ & 8100 & 8,105 & 2,963 & 1,493 & 1,789 & 1,030\\
      \hline
    \end{tabular}
  \end{center}
\end{example}
\vspace{6pt}

\begin{example}[Plato Corpus Size]
  The complete Plato corpus \ref{ex:corp-plato} consists of 25 documents and, each tokenized the same way as before, has the word count $\sum_{d\in C_P}\#W_d=670,936$ and the vocabulary size $\#V(C_P)=19,432$.
\end{example}
\vspace{6pt}

\subsection{Term-Document \& Term-Term Matricies}

\begin{definition}[Document Word Occurences]
  The number of times a word-token $w$ occurs in a tokenized document $W_d$ is given by $\operatorname{count}(w,W_d)$.
\end{definition}

\begin{definition}[Term Document Matrix]
  A term-document matrix $X$, (for corpus $C$ and tokenizer $T$) has columns corresponding to each corpus document, and rows corresponding to each word in the corpus vocabulary (hence having dimensions $\#V(C)\times \#C$). The matrix entry 
\end{definition}

\subsection{Term Weighting}

\subsection{Smoothing}

\subsection{Singular Value Decomposition}

\section{Predict Models}

\subsection{Early Developments}

\subsection{$\texttt{word2vec}$: Skip-Gram with Negative Sampling}

\subsection{$\texttt{word2vec}$: Continuous Bag of Words}

\section{Measuring Similarity}

\section{Comparison}

\section{Other Approaches}
